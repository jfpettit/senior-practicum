{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Chapters 1 - 3 in Sutton RL book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is my summary and paraphrasing of Chapter 1 in Sutton and Barto's Reinforcement Learning: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal.\" For tic-tac-toe this would be something like learning how to take actions to maximize a win rate.\n",
    "\n",
    "\"A learning agent must be able to sense the state of its environment to some extent and must be able to take some actions to affect the state.\" In terms of tic-tac-toe this means the agent needs to be able to observe the state of the board and it needs to be able to place Xs and Os where it chooses.\n",
    "\n",
    "\"The agent must have a goal or goals relating to the state of the environment\"; here the goal would be to win.\n",
    "\n",
    "\"Exploration vs Exploitation - To get a lot of reward, an agent has to use actions it has used in the past that it knows produces reward, but to find reward giving actions, an agent must try actions its never tried before. So the agent has to try a variety of actions and progressively favor those that appear best.\" In tic-tac-toe this could be equivalent to continually trying new moves in hopes of uncovering a better strategy than what has already been found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 main subelements of a reinforcement learning system:\n",
    "    - Policy\n",
    "    - Reward signal\n",
    "    - Value function\n",
    "    - Environment model (optional)\n",
    "\n",
    "\"Policy describes the learning agent's way of behaving at a given time, it provides a mapping from the state of the environment to what action to take in that state.\" In tic-tac-toe, the policy would describe what move to make depending on the state of the board.\n",
    "\n",
    "\"The reward signal defines the goal in an RL problem.\" At each time step, the environment sends a reward value to the agent. The agent's goal is to maximize the reward it receives over long period of time. In tic-tac-toe, the reward could come from winning, so the agent would want to make moves that are winning moves so that it can receive more reward.\n",
    "\n",
    "\"Since the reward signal operates at each time step, it is a short-term value; the value function declares what is good in the long run.\" The value of a state is the total reward an agent can expect to gather over the future, starting with that state. Values indicate the long-term desirability of states, taking into account the states that are likely to follow, and the available reward in those states. In tic-tac-toe, a high value move might be one that likely leads to a winning sequence of moves.\n",
    "\n",
    "\"When we are making action choices, we use values as judgment, not rewards.\" We want actions that yield the highest value, not the highest reward, because these are the best actions over the long run. It's much harder to determine values than rewards; rewards are given to us by the environment, while values must be estimated using an agent's past experiences and observations. \n",
    "\n",
    "\"The model of the environment mimics the environment, or it allows inferences to be made about how the environment will behave.\" i.e. given a state and action, a model might be able to predict the next state and reward. We use models for planning. In tic-tac-toe, this might mean a model of the board and the other player where a move can be made in the model and the model tries to return a prediction of the opponents responding move and the reward from that next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tic-tac-toe example. \n",
    "\n",
    "Let's consider draws and losses to be equally bad. How can we build a player that will find the imperfections in its opponent's play and learn to maximize its chances of winning?\n",
    "\n",
    "This can't be solved readily through classical techniques. The minimax solution from game theory doesn't work because it asssumes a particular way of playing by the opponent, and something like dynamic programming requires as input complete specifications of the opponent, including the probabilities of each move the opponent could make in each board state. \n",
    "\n",
    "We could play lots of games against the opponent and learn some model of the opponents behavior, and then apply dynamic programming to those estimated move probabilities from the model to compute the optimal moves.\n",
    "\n",
    "An evolutionary method would search the space of possible policies for one with a high probability of winning against the opponent. In this case, the policy is a rule that tells the player what move to make for every state of the game.\n",
    "\n",
    "Tic-tac-toe with method making use of a value function:\n",
    "\n",
    "We set up a table of numbers, one for each state of the game. Each number is the latest estimate of the probability of winning from that state. This estimate is considered the state's value, and the whole table is the learned value function. \n",
    "\n",
    "If we are playing X's, then each state where there are 3 X's in a row has a probability of 1, since we've already won, and each state where there are 3 O's in a row or the board is full has a probability of 0, since we've either lost or drawn and we can't win from that state. Make the initial values of all other states .5, to guess an initial 50% probability of winning.\n",
    "\n",
    "Play lots of games against the opponent. To pick our next move we look at the states that would result from each possible move and check the corresponding win probability in the table. Most often we move greedily, picking the move that leads to the state with the highest value. But sometimes we randomly select a move from the table, and these are our exploratory moves, which allow us to see states that we might otherwise never find. \n",
    "\n",
    "While playing, we change the values of the states we wind up in. We try to make them more accurate estimates of the probability of winning. The current value of the earlier state is updated to be closer to the value of the later state. This is done by moving the value of the earlier state a fraction of the way towards the value of the later state.\n",
    "    - Let s be the state before the greedy move\n",
    "    - s' is the state after the greedy move\n",
    "    - V(s) is the update to the estimated value of s\n",
    "    - alpha is learning rate\n",
    "    \n",
    "V(s) <-- V(s) + alpha * [V(s') - V(s)]\n",
    "\n",
    "This update rule is an example of temporal difference learning, because its changes are based on a difference at two different times (V(s') - V(s)).\n",
    "\n",
    "If alpha is reduced to zero over time, this method converges for any fixed player to the true probabilities of winning for any given state, given optimal play by the method. If alpha isn't reduced all the way to zero over time, then this method also plays well against players that slowly change their style of playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-tac-toe code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for search, use current state and pretend to place an X in every available spot, then, retrieve the value for each resulting fake state\n",
    "\n",
    "# for generating the dataframe\n",
    "# use the 9 nested for loops \n",
    "# at each step, check to make sure the generated row is legal\n",
    "# write a function stating whether the game was won by X or O or drawn and then return 0, .5 or 1. Use this to build the value table\n",
    "class tictactoe:\n",
    "    def __init__(self):\n",
    "        self.x_win = False\n",
    "        self.o_win = False\n",
    "        self.game_over = False\n",
    "        self.v = dict()\n",
    "        self.chars = [0, 1, 2]\n",
    "        \n",
    "    def gen_state_arr(self):\n",
    "        # 0 is empty, 1 is X, 2 is O\n",
    "        arr = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                for k in range(3):\n",
    "                    for t in range(3):\n",
    "                        for u in range(3):\n",
    "                            for s in range(3):\n",
    "                                for q in range(3):\n",
    "                                    for y in range(3):\n",
    "                                        for z in range(3):\n",
    "                                            tmp = np.array([self.chars[i], self.chars[j], self.chars[k], self.chars[t], self.chars[u], self.chars[s],\n",
    "                                                       self.chars[q], self.chars[y], self.chars[z]]).reshape(3, 3)\n",
    "                                            if self.check_legal(tmp) == True:\n",
    "                                                arr.append(tmp)\n",
    "                                                \n",
    "        return np.asarray(arr)\n",
    "        \n",
    "    def gen_value_arr(self, state_arr):\n",
    "        vals = []\n",
    "        for i in range(len(state_arr)):\n",
    "            value = self.check_over(state_arr[i])\n",
    "            vals.append(value)\n",
    "        vals = np.asarray(vals)\n",
    "        vals = vals[vals < 3]\n",
    "        return vals\n",
    "    \n",
    "    def check_legal(self, state):\n",
    "        x_count, o_count, empty_count = 0, 0, 0\n",
    "        state_cpy = np.copy(state).ravel()\n",
    "        for i in range(len(state_cpy)):\n",
    "            if state_cpy[i] == 1: x_count += 1\n",
    "            elif state_cpy[i] == 2: o_count += 1\n",
    "            elif state_cpy[i] == 0: empty_count += 1\n",
    "        \n",
    "        if x_count == o_count + 1 or x_count == o_count:\n",
    "            # check for X and O win\n",
    "            if self.check_over(state) == 3:\n",
    "                return False\n",
    "            # check for O win and num Xs = num Os\n",
    "            if x_count != o_count and self.check_over(state) == 0:\n",
    "                return False\n",
    "            # check for X win and num Xs = num Os + 1\n",
    "            if self.check_over(state) == 1 and x_count != o_count + 1:\n",
    "                return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "        \n",
    "    def check_over(self, state):\n",
    "        x_vec = np.ones(3, dtype=int)\n",
    "        o_vec = np.full(3, 2, dtype=int)\n",
    "        x_win, o_win = 0, 0\n",
    "        \n",
    "        state_diag = np.diag(state).astype(int)\n",
    "        state_rev_diag = np.fliplr(state).diagonal().astype(int)\n",
    "        \n",
    "        if np.array_equal(state_diag, x_vec) or np.array_equal(state_rev_diag, x_vec):\n",
    "            x_win += 1\n",
    "        \n",
    "        if np.array_equal(state_diag, o_vec) or np.array_equal(state_rev_diag, o_vec):\n",
    "            o_win += 1\n",
    "        \n",
    "        cols = [state[:, 0].astype(int), state[:, 1].astype(int), state[:, 2].astype(int)]\n",
    "        rows = [state[0, :].astype(int), state[1, :].astype(int), state[2, :].astype(int)]\n",
    "        \n",
    "        for i in cols:\n",
    "            if np.array_equal(i, x_vec):\n",
    "                x_win += 1\n",
    "            elif np.array_equal(i, o_vec):\n",
    "                o_win += 1\n",
    "        \n",
    "        for i in rows:\n",
    "            if np.array_equal(i, x_vec):\n",
    "                x_win += 1\n",
    "            elif np.array_equal(i, o_vec):\n",
    "                o_win += 1\n",
    "        \n",
    "        if x_win >= 1 and o_win == 1:\n",
    "            return 3\n",
    "        elif x_win >= 1:\n",
    "            return 1\n",
    "        elif o_win == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    def human_viz(self, state):\n",
    "        tmp_board = np.empty(state.shape, dtype=str)\n",
    "        for i in range(len(state)):\n",
    "            for j in range(len(state[i])):\n",
    "                if state[i][j] == 1:\n",
    "                    tmp_board[i][j] = 'X'\n",
    "                elif state[i][j] == 2:\n",
    "                    tmp_board[i][j] = 'O'\n",
    "                elif state[i][j] == 0:\n",
    "                    tmp_board[i][j] = '-'\n",
    "        print(tmp_board)\n",
    "        \n",
    "    def find_next_move_slow(self, state_arr, state, player):\n",
    "        tmp_states = []\n",
    "        inds = []\n",
    "        if player is 1:\n",
    "            for i in range(len(state)):\n",
    "                for j in range(len(state[i])):\n",
    "                    tmp = np.copy(state)\n",
    "                    if tmp[i][j] == 0:\n",
    "                        tmp[i][j] = 1\n",
    "                        tmp_states.append(tmp)\n",
    "            \n",
    "            for i in range(len(tmp_states)):\n",
    "                for j in range(len(state_arr)):\n",
    "                    if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "                        inds.append(j)\n",
    "            return inds\n",
    "        \n",
    "        if player is 2:\n",
    "            for i in range(len(state)):\n",
    "                for j in range(len(state[i])):\n",
    "                    tmp = np.copy(state)\n",
    "                    if tmp[i][j] == 0:\n",
    "                        tmp[i][j] = 2\n",
    "                        tmp_states.append(tmp)\n",
    "                        \n",
    "            for i in range(len(tmp_states)):\n",
    "                for j in range(len(state_arr)):\n",
    "                    if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "                        inds.append(j)\n",
    "            return inds\n",
    "    \n",
    "    def find_next_move(self, state_arr, state, player):\n",
    "        tmp_states = []\n",
    "        inds = []\n",
    "        sort_inds = np.asarray(np.where(state == 0))\n",
    "        if player is 1:\n",
    "            for i in range(sort_inds.shape[1]):\n",
    "                tmp = np.copy(state)\n",
    "                tmp_ind = sort_inds[:, i]\n",
    "                tmp[tmp_ind[0], tmp_ind[1]] = 1\n",
    "                #tmp_states.append(tmp)\n",
    "                inds.append(np.argmax((tmp == state_arr).sum(axis=1).sum(axis=1)))\n",
    "            \n",
    "            #for i in range(len(tmp_states)):\n",
    "            #    for j in range(len(state_arr)):\n",
    "            #        if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "            #            inds.append(j)\n",
    "            return inds\n",
    "        \n",
    "        if player is 2:\n",
    "            for i in range(sort_inds.shape[1]):\n",
    "                tmp = np.copy(state)\n",
    "                tmp_ind = sort_inds[:, i]\n",
    "                tmp[tmp_ind[0], tmp_ind[1]] = 2\n",
    "                #tmp_states.append(tmp)\n",
    "                inds.append(np.argmax((tmp == state_arr).sum(axis=1).sum(axis=1)))\n",
    "                        \n",
    "            #for i in range(len(tmp_states)):\n",
    "            #    for j in range(len(state_arr)):\n",
    "            #        if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "            #            inds.append(j)\n",
    "            return inds\n",
    "        \n",
    "    def get_current_state_index(self, state, state_arr):\n",
    "        for i in range(len(state_arr)):\n",
    "            if np.array_equal(state_arr[i], state):\n",
    "                return state, i\n",
    "        return 'Current state invalid.'\n",
    "    \n",
    "    def human_play(self, state_arr, td_values, agent):\n",
    "        state = state_arr[0]\n",
    "        iter_count = 0\n",
    "        while self.check_over(state) == 0.5 and iter_count <= 8:\n",
    "            cur_state_ind = self.get_current_state_index(state, state_arr)\n",
    "            if iter_count % 2 == 0:\n",
    "                move_inds = self.find_next_move(state_arr, state, 1)\n",
    "                new_move_ind = agent.get_move_update_values(move_inds, cur_state_ind[1], .1)\n",
    "                state = state_arr[new_move_ind]\n",
    "            else:\n",
    "                self.human_viz(state)\n",
    "                move_valid = False\n",
    "                while move_valid == False:\n",
    "                    human_move = input('Input your move coordinates, separated by a comma: ')\n",
    "                    coord_1, coord_2 = int(human_move[0]), int(human_move[-1])\n",
    "                    if state[coord_1, coord_2] != 0:\n",
    "                        print('Invalid move, try again.')\n",
    "                        pass\n",
    "                    tmp_state = np.copy(state)\n",
    "                    tmp_state[coord_1, coord_2] = 2\n",
    "                    if self.check_legal(tmp_state) == True:\n",
    "                        state = tmp_state\n",
    "                        move_valid = True\n",
    "            \n",
    "            iter_count += 1\n",
    "        self.human_viz(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = tictactoe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ttt.gen_state_arr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = ttt.gen_value_arr(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.asarray(vals)\n",
    "(v == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(v==1).sum() + (v==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight update rule: V(s) <-- V(s) + alpha * [V(s') - V(s)]\n",
    "\n",
    "class td_agent:\n",
    "    def __init__(self, values, player):\n",
    "        self.vals = values\n",
    "        self.player = player\n",
    "                \n",
    "    def get_move_update_values(self, move_inds, current_state_ind, alpha):\n",
    "        if self.player == 1:\n",
    "            greedy_move = np.argmax(self.vals[move_inds])\n",
    "        elif self.player == 2:\n",
    "            greedy_move = np.argmin(self.vals[move_inds])\n",
    "\n",
    "        if np.random.binomial(1, .1) == 1:\n",
    "            move_ind = np.random.randint(0, len(move_inds))\n",
    "            #print('random move: ', move_ind)\n",
    "            move = move_inds[move_ind]\n",
    "        else:\n",
    "            move = move_inds[greedy_move]\n",
    "        self.vals[move] += alpha * (self.vals[current_state_ind] - self.vals[move])\n",
    "        \n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = tictactoe()\n",
    "state_arr = game.gen_state_arr()\n",
    "values = game.gen_value_arr(state_arr)\n",
    "\n",
    "o_vals = np.load('/home/jfp15b/Documents/reinforcement-learning/TD_TicTacToe/o_game_vals.npy')\n",
    "x_vals = np.load('/home/jfp15b/Documents/reinforcement-learning/TD_TicTacToe/x_game_vals.npy')\n",
    "\n",
    "TD_x = td_agent(x_vals, 1)\n",
    "TD_o = td_agent(o_vals, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts1 = state_arr[0]\n",
    "# win probability should be 1\n",
    "ts2 = np.array([1, 0, 1, 2, 2, 0, 1, 0, 2]).reshape(3, 3)\n",
    "ts3 = np.array([1, 0, 1, 2, 1, 2, 0, 2, 0]).reshape(3, 3)\n",
    "ts4 = np.array([1, 2, 1, 2, 0, 2, 1, 2, 1]).reshape(3, 3)\n",
    "ts5 = np.array([2, 0, 0, 2, 1, 0, 1, 1, 2]).reshape(3, 3)\n",
    "# win probability should be 0\n",
    "ts6 = np.array([2, 0, 2, 0, 1, 1, 2, 1, 1]).reshape(3, 3)\n",
    "ts7 = np.array([2, 2, 0, 1, 0, 0, 1, 0, 1]).reshape(3, 3)\n",
    "ts8 = np.array([2, 0, 2, 1, 2, 1, 1, 1, 0]).reshape(3, 3)\n",
    "ts9 = np.array([1, 1, 0, 1, 2, 0, 2, 0, 0]).reshape(3, 3)\n",
    "ts10 = np.array([1, 1, 0, 0, 1, 0, 2, 2, 0]).reshape(3, 3)\n",
    "\n",
    "ts_inds = []\n",
    "ts_ = [ts1, ts2, ts3, ts4, ts5, ts6, ts7, ts8, ts9, ts10]\n",
    "for i in ts_:\n",
    "    ts_inds.append(game.get_current_state_index(i, state_arr)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X values for test states:  [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "for i in range(150000):\n",
    "    iter_count = 0\n",
    "    cur_state = state_arr[0]\n",
    "    while game.check_over(cur_state) == 0.5 and iter_count <= 8:\n",
    "        cur_state_ind = game.get_current_state_index(cur_state, state_arr)\n",
    "        if iter_count % 2 == 0:\n",
    "            move_inds = game.find_next_move(state_arr, cur_state, 1)\n",
    "            new_move_ind = TD_x.get_move_update_values(move_inds, cur_state_ind[1], .1)\n",
    "            cur_state = state_arr[new_move_ind]\n",
    "        else:\n",
    "            move_inds = game.find_next_move(state_arr, cur_state, 2)\n",
    "            new_move_ind = TD_o.get_move_update_values(move_inds, cur_state_ind[1], .1)\n",
    "            cur_state = state_arr[new_move_ind]\n",
    "        iter_count += 1\n",
    "    if i%10000 == 0:\n",
    "        print('X values for test states: ', TD_x.vals[ts_inds])\n",
    "        np.save('x_game_vals.npy', TD_x.vals)\n",
    "        np.save('o_game_vals.npy', TD_o.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  2, 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'X' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' 'O']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  0, 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'X' 'O']\n",
      " ['X' '-' '-']\n",
      " ['-' '-' 'O']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  2, 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'X' 'O']\n",
      " ['X' 'X' '-']\n",
      " ['O' '-' 'O']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  2, 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'X' 'O']\n",
      " ['X' 'X' '-']\n",
      " ['O' 'O' 'O']]\n"
     ]
    }
   ],
   "source": [
    "game.human_play(state_arr, x_vals, TD_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "-Need to conduct checks of find_next_move function to try and make sure it is actually finding valid moves.\n",
    "\n",
    "-Need to write the TD learning portion \n",
    "\n",
    "-Come up with way to play against it\n",
    "\n",
    "-Come up with visualization of algorithm playing\n",
    "\n",
    "-Train it\n",
    "\n",
    "-Play against it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "The following is my notes, summary, and paraphrasing of Chapter 2 in Sutton and Barto's Reinforcement Learning: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits\n",
    "\n",
    "\"The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that $evaluates$ the actions taken rather than $instructs$ by giving the correct actions.\"\n",
    "\n",
    "If we used feedback that is only evaluating each move, this will tell us how good the action taken was, but not if it was the best or worst action we could've possibly taken.\n",
    "\n",
    "Using feedback that is just instructive will only tell us the right action to take, regardless of the action that we took."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: A $k$-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem:\n",
    "\n",
    "We repeatedly have to choose from among $k$ different actions. After each action, a reward is given, the reward is picked from an unchanging probability distribution that depends on the action chosen. The goal is to maximize the (expected) total reward over an arbitrary time period.\n",
    "\n",
    "\"This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or \"one armed bandit\", except that it has $k$ levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are payoffs for hitting the jackpot. Through repeated action selections you are to maximize your chances of winning by concentrating your actions on the best levers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
