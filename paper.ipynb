{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Senior Practicum in Scientific Computing\n",
    "\n",
    "### Jacob Pettit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "\n",
    "#### What is RL?\n",
    " - Components of RL system\n",
    "     - Policy\n",
    "     - Reward system\n",
    "     - Value function\n",
    "     - Model of environment\n",
    " - K-armed bandits\n",
    " - Markov Decision Processes\n",
    " - Policies and Value functions\n",
    " \n",
    "#### Sampling of algorithms\n",
    " - TD learning for tic-tac-toe\n",
    " - Dynamic programming for policy iteration in gridworld\n",
    " - Policy gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning lets us make machines learn to take actions in an environment that will maximize a reward. We do this by learning how to map from states of an environment to actions in such a way that we maximize the reward signal. Our agent isn't told which actions to take, instead, it must learn which actions will give the most reward by trying them. An intelligent agent must be able to sense its environment. It also must be able to take actions that affect the state of the environment. In some cases, the actions an agent takes might effect not only its current situation, but also the next situation and subsequently all future reward. Optimal action search by trial and error and learning from delayed rewards are the most important distinguishing features of RL. Reinforcement Learning is all at once a problem, a set of methods that work on the problem, and the field studying the problem and the methods. \n",
    "\n",
    "A challenge that arises in RL and not in other types of learning like supervised or unsupervised learning is that of dealing with a trade off between exploration and exploitation. In order to accumulate a lot of reward, an agent must choose actions it has chosen in the past that have given it reward. However, in order to discover actions that give lots of reward, the agent has to experiment with actions it has never tried before. So, the agent has to *exploit* actions it has already tried that it knows give it some reward, and it also has to *explore* other actions so that it can learn to make even better action choices in the future. We cannot exclusively pursue exploration or exploitation and succeed at the task. Our agent has to both explore new actions and exploit which actions it has already learned will give good reward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
