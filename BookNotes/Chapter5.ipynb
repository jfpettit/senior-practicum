{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 Monte Carlo Methods\n",
    "\n",
    "In contrast to Chapter 4, we don't assume to have complete knowledge of the environment here. Monte Carlo (MC) methods only need \"experience (sample sequences of states, actions, and rewards from actual (or simulated) interaction with an environment.\" Learning from actual experience is a big deal because then no knowledge of the mechanics and dynamics of the environment is needed to learn optimal behaviors. When we learn from simulated experience, this is also very useful becauase although we need a model of the environment, we only need to use the model to generate transition probabilities for samples, not for every possible transition like dynamic programming requires. \n",
    "\n",
    "\"Monte Carlo methods enable us to solve the RL problem by averaging sample returns.\" We'll only look at Monte Carlo methods for episodic tasks so that we can be certain that we're only dealing with well-defined returns. Policies and value estimates are only changed at the end of an episode. \n",
    "\n",
    "MC samples and averages future returns over state-action pairs. This is similar to the k-armed bandit methods from Chapter 2; the bandit methods sampled and averaged reward for each action. The big difference now is that we have more than one state, and that we allow the states to interact with each other. Recall, this is the full RL problem alluded to at the end of Chapter 2 and covered throughout Chapter 3. \"The return after taking an action in one state depends on the actions taken in later states in the same episode.\" This problem becomes nonstationary because we are continuously learning to make different action choices. \n",
    "\n",
    "We adapt the idea of Generalized Policy Iteration (GPI) to handle the nonstationarity of the problem; we use samples from the MDP to learn the value function. This is in contrast to when dynamic programming was used to directly compute the value function using GPI. Each piece of GPI is extended from dynamic programming to MC, where here we use sample experience to learn the policy $\\pi$ and $v_\\pi$ and $q_\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1: Monte Carlo Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
