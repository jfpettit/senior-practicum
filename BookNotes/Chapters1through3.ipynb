{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Chapters 1 - 3 in Sutton RL book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is my summary and paraphrasing of Chapter 1 in Sutton and Barto's Reinforcement Learning: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Reinforcement learning is learning what to do - how to map situations to actions - so as to maximize a numerical reward signal.\" For tic-tac-toe this would be something like learning how to take actions to maximize a win rate.\n",
    "\n",
    "\"A learning agent must be able to sense the state of its environment to some extent and must be able to take some actions to affect the state.\" In terms of tic-tac-toe this means the agent needs to be able to observe the state of the board and it needs to be able to place Xs and Os where it chooses.\n",
    "\n",
    "\"The agent must have a goal or goals relating to the state of the environment\"; here the goal would be to win.\n",
    "\n",
    "\"Exploration vs Exploitation - To get a lot of reward, an agent has to use actions it has used in the past that it knows produces reward, but to find reward giving actions, an agent must try actions its never tried before. So the agent has to try a variety of actions and progressively favor those that appear best.\" In tic-tac-toe this could be equivalent to continually trying new moves in hopes of uncovering a better strategy than what has already been found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 main subelements of a reinforcement learning system:\n",
    "    - Policy\n",
    "    - Reward signal\n",
    "    - Value function\n",
    "    - Environment model (optional)\n",
    "\n",
    "\"Policy describes the learning agent's way of behaving at a given time, it provides a mapping from the state of the environment to what action to take in that state.\" In tic-tac-toe, the policy would describe what move to make depending on the state of the board.\n",
    "\n",
    "\"The reward signal defines the goal in an RL problem.\" At each time step, the environment sends a reward value to the agent. The agent's goal is to maximize the reward it receives over long period of time. In tic-tac-toe, the reward could come from winning, so the agent would want to make moves that are winning moves so that it can receive more reward.\n",
    "\n",
    "\"Since the reward signal operates at each time step, it is a short-term value; the value function declares what is good in the long run.\" The value of a state is the total reward an agent can expect to gather over the future, starting with that state. Values indicate the long-term desirability of states, taking into account the states that are likely to follow, and the available reward in those states. In tic-tac-toe, a high value move might be one that likely leads to a winning sequence of moves.\n",
    "\n",
    "\"When we are making action choices, we use values as judgment, not rewards.\" We want actions that yield the highest value, not the highest reward, because these are the best actions over the long run. It's much harder to determine values than rewards; rewards are given to us by the environment, while values must be estimated using an agent's past experiences and observations. \n",
    "\n",
    "\"The model of the environment mimics the environment, or it allows inferences to be made about how the environment will behave.\" i.e. given a state and action, a model might be able to predict the next state and reward. We use models for planning. In tic-tac-toe, this might mean a model of the board and the other player where a move can be made in the model and the model tries to return a prediction of the opponents responding move and the reward from that next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tic-tac-toe example. \n",
    "\n",
    "Let's consider draws and losses to be equally bad. How can we build a player that will find the imperfections in its opponent's play and learn to maximize its chances of winning?\n",
    "\n",
    "This can't be solved readily through classical techniques. The minimax solution from game theory doesn't work because it asssumes a particular way of playing by the opponent, and something like dynamic programming requires as input complete specifications of the opponent, including the probabilities of each move the opponent could make in each board state. \n",
    "\n",
    "We could play lots of games against the opponent and learn some model of the opponents behavior, and then apply dynamic programming to those estimated move probabilities from the model to compute the optimal moves.\n",
    "\n",
    "An evolutionary method would search the space of possible policies for one with a high probability of winning against the opponent. In this case, the policy is a rule that tells the player what move to make for every state of the game.\n",
    "\n",
    "Tic-tac-toe with method making use of a value function:\n",
    "\n",
    "We set up a table of numbers, one for each state of the game. Each number is the latest estimate of the probability of winning from that state. This estimate is considered the state's value, and the whole table is the learned value function. \n",
    "\n",
    "If we are playing X's, then each state where there are 3 X's in a row has a probability of 1, since we've already won, and each state where there are 3 O's in a row or the board is full has a probability of 0, since we've either lost or drawn and we can't win from that state. Make the initial values of all other states .5, to guess an initial 50% probability of winning.\n",
    "\n",
    "Play lots of games against the opponent. To pick our next move we look at the states that would result from each possible move and check the corresponding win probability in the table. Most often we move greedily, picking the move that leads to the state with the highest value. But sometimes we randomly select a move from the table, and these are our exploratory moves, which allow us to see states that we might otherwise never find. \n",
    "\n",
    "While playing, we change the values of the states we wind up in. We try to make them more accurate estimates of the probability of winning. The current value of the earlier state is updated to be closer to the value of the later state. This is done by moving the value of the earlier state a fraction of the way towards the value of the later state.\n",
    "    - Let s be the state before the greedy move\n",
    "    - s' is the state after the greedy move\n",
    "    - V(s) is the update to the estimated value of s\n",
    "    - alpha is learning rate\n",
    "    \n",
    "V(s) <-- V(s) + alpha * [V(s') - V(s)]\n",
    "\n",
    "This update rule is an example of temporal difference learning, because its changes are based on a difference at two different times (V(s') - V(s)).\n",
    "\n",
    "If alpha is reduced to zero over time, this method converges for any fixed player to the true probabilities of winning for any given state, given optimal play by the method. If alpha isn't reduced all the way to zero over time, then this method also plays well against players that slowly change their style of playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic-tac-toe code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- Fix gen_o_values function::DONE\n",
    "- Start making changes to make more general for n by n tic-tac-toe\n",
    "- Clean up code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tictactoe:\n",
    "    def __init__(self):\n",
    "        self.x_win = False\n",
    "        self.o_win = False\n",
    "        self.game_over = False\n",
    "        self.v = dict()\n",
    "        self.chars = [0, 1, 2]\n",
    "        \n",
    "    def gen_3_state_arr(self):\n",
    "        arr = []\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                for k in range(3):\n",
    "                    for t in range(3):\n",
    "                        for u in range(3):\n",
    "                            for s in range(3):\n",
    "                                for q in range(3):\n",
    "                                    for y in range(3):\n",
    "                                        for z in range(3):\n",
    "                                            tmp = np.array([self.chars[i], self.chars[j], self.chars[k], self.chars[t], self.chars[u], self.chars[s],\n",
    "                                                       self.chars[q], self.chars[y], self.chars[z]]).reshape(3, 3)\n",
    "                                            if self.check_legal(tmp, 3) == True:\n",
    "                                                arr.append(tmp)\n",
    "                                                \n",
    "        return np.asarray(arr)\n",
    "    \n",
    "    def gen_4_state_arr(self):\n",
    "        arr = []\n",
    "        for a in range(3):\n",
    "            for b in range(3):\n",
    "                for c in range(3):\n",
    "                    for d in range(3):\n",
    "                        for e in range(3):\n",
    "                            for f in range(3):\n",
    "                                for g in range(3):\n",
    "                                    for h in range(3):\n",
    "                                        for i in range(3):\n",
    "                                            for j in range(3):\n",
    "                                                for k in range(3):\n",
    "                                                    for l in range(3):\n",
    "                                                        for m in range(3):\n",
    "                                                            for n in range(3):\n",
    "                                                                for o in range(3):\n",
    "                                                                    for p in range(3):\n",
    "                                                                        tmp = np.array([self.chars[a], self.chars[b], self.chars[c], self.chars[d], self.chars[e],\n",
    "                                                                                        self.chars[f], self.chars[g], self.chars[h], self.chars[i], self.chars[j],\n",
    "                                                                                        self.chars[k], self.chars[l], self.chars[m], self.chars[n], self.chars[o],\n",
    "                                                                                        self.chars[p]]).reshape(4, 4)\n",
    "                                                                        if self.check_legal(tmp, 4) == True:\n",
    "                                                                            arr.append(tmp)\n",
    "        return np.asarray(arr)\n",
    "                                            \n",
    "    \n",
    "    def gen_x_values(self, state_arr):\n",
    "        vals = []\n",
    "        for i in range(len(state_arr)):\n",
    "            value = self.check_over(state_arr[i], n)\n",
    "            vals.append(value)\n",
    "        vals = np.asarray(vals)\n",
    "        vals = vals[vals < 3]\n",
    "        return vals\n",
    "    \n",
    "    def gen_o_values(self, x_values):\n",
    "        return 1 - x_values\n",
    "    \n",
    "    def check_legal(self, state, n):\n",
    "        x_count, o_count, empty_count = 0, 0, 0\n",
    "        state_cpy = np.copy(state).ravel()\n",
    "        for i in range(len(state_cpy)):\n",
    "            if state_cpy[i] == 1: x_count += 1\n",
    "            elif state_cpy[i] == 2: o_count += 1\n",
    "            elif state_cpy[i] == 0: empty_count += 1\n",
    "        \n",
    "        if x_count == o_count + 1 or x_count == o_count:\n",
    "            # check for X and O win\n",
    "            if self.check_over(state, n) == 3:\n",
    "                return False\n",
    "            # check for O win and num Xs = num Os\n",
    "            if x_count != o_count and self.check_over(state, n) == 0:\n",
    "                return False\n",
    "            # check for X win and num Xs = num Os + 1\n",
    "            if self.check_over(state, n) == 1 and x_count != o_count + 1:\n",
    "                return False\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "        \n",
    "    def check_over(self, state, n):\n",
    "        x_vec = np.ones(n, dtype=int)\n",
    "        o_vec = np.full(n, 2, dtype=int)\n",
    "        x_win, o_win = 0, 0\n",
    "        \n",
    "        state_diag = np.diag(state).astype(int)\n",
    "        state_rev_diag = np.fliplr(state).diagonal().astype(int)\n",
    "        \n",
    "        if np.array_equal(state_diag, x_vec) or np.array_equal(state_rev_diag, x_vec):\n",
    "            x_win += 1\n",
    "        \n",
    "        if np.array_equal(state_diag, o_vec) or np.array_equal(state_rev_diag, o_vec):\n",
    "            o_win += 1\n",
    "        \n",
    "        cols = [state[:, i].astype(int) for i in range(len(state))]\n",
    "        #cols = [state[:, 0].astype(int), state[:, 1].astype(int), state[:, 2].astype(int)]\n",
    "        #rows = [state[0, :].astype(int), state[1, :].astype(int), state[2, :].astype(int)]\n",
    "        rows = [state[i, :].astype(int) for i in range(len(state))]\n",
    "        \n",
    "        for i in cols:\n",
    "            if np.array_equal(i, x_vec):\n",
    "                x_win += 1\n",
    "            elif np.array_equal(i, o_vec):\n",
    "                o_win += 1\n",
    "        \n",
    "        for i in rows:\n",
    "            if np.array_equal(i, x_vec):\n",
    "                x_win += 1\n",
    "            elif np.array_equal(i, o_vec):\n",
    "                o_win += 1\n",
    "        \n",
    "        if x_win >= 1 and o_win == 1:\n",
    "            return 3\n",
    "        elif x_win >= 1:\n",
    "            return 1\n",
    "        elif o_win == 1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 0.5\n",
    "    \n",
    "    def human_viz(self, state):\n",
    "        tmp_board = np.empty(state.shape, dtype=str)\n",
    "        for i in range(len(state)):\n",
    "            for j in range(len(state[i])):\n",
    "                if state[i][j] == 1:\n",
    "                    tmp_board[i][j] = 'X'\n",
    "                elif state[i][j] == 2:\n",
    "                    tmp_board[i][j] = 'O'\n",
    "                elif state[i][j] == 0:\n",
    "                    tmp_board[i][j] = '-'\n",
    "        print(tmp_board)\n",
    "        \n",
    "    def find_next_move_slow(self, state_arr, state, player):\n",
    "        tmp_states = []\n",
    "        inds = []\n",
    "        if player is 1:\n",
    "            for i in range(len(state)):\n",
    "                for j in range(len(state[i])):\n",
    "                    tmp = np.copy(state)\n",
    "                    if tmp[i][j] == 0:\n",
    "                        tmp[i][j] = 1\n",
    "                        tmp_states.append(tmp)\n",
    "            \n",
    "            for i in range(len(tmp_states)):\n",
    "                for j in range(len(state_arr)):\n",
    "                    if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "                        inds.append(j)\n",
    "            return inds\n",
    "        \n",
    "        if player is 2:\n",
    "            for i in range(len(state)):\n",
    "                for j in range(len(state[i])):\n",
    "                    tmp = np.copy(state)\n",
    "                    if tmp[i][j] == 0:\n",
    "                        tmp[i][j] = 2\n",
    "                        tmp_states.append(tmp)\n",
    "                        \n",
    "            for i in range(len(tmp_states)):\n",
    "                for j in range(len(state_arr)):\n",
    "                    if np.array_equal(tmp_states[i], state_arr[j]):\n",
    "                        inds.append(j)\n",
    "            return inds\n",
    "    \n",
    "    def find_next_move(self, state_arr, state, player):\n",
    "        tmp_states = []\n",
    "        inds = []\n",
    "        sort_inds = np.asarray(np.where(state == 0))\n",
    "        if player is 1:\n",
    "            for i in range(sort_inds.shape[1]):\n",
    "                tmp = np.copy(state)\n",
    "                tmp_ind = sort_inds[:, i]\n",
    "                tmp[tmp_ind[0], tmp_ind[1]] = 1\n",
    "                inds.append(np.argmax((tmp == state_arr).sum(axis=1).sum(axis=1)))\n",
    "            return inds\n",
    "        \n",
    "        if player is 2:\n",
    "            for i in range(sort_inds.shape[1]):\n",
    "                tmp = np.copy(state)\n",
    "                tmp_ind = sort_inds[:, i]\n",
    "                tmp[tmp_ind[0], tmp_ind[1]] = 2\n",
    "                inds.append(np.argmax((tmp == state_arr).sum(axis=1).sum(axis=1)))\n",
    "            return inds\n",
    "        \n",
    "    def get_current_state_index(self, state, state_arr):\n",
    "        for i in range(len(state_arr)):\n",
    "            if np.array_equal(state_arr[i], state):\n",
    "                return state, i\n",
    "        return 'Current state invalid.'\n",
    "    \n",
    "    def human_play(self, state_arr, agent, agent_char):\n",
    "        state = state_arr[0]\n",
    "        iter_count = 0\n",
    "        if agent_char is 1:\n",
    "            while self.check_over(state, 3) == 0.5 and iter_count <= 8:\n",
    "                cur_state_ind = self.get_current_state_index(state, state_arr)\n",
    "                if iter_count % 2 == 0:\n",
    "                    move_inds = self.find_next_move(state_arr, state, 1)\n",
    "                    new_move_ind = agent.exploit_learned_probs(move_inds, cur_state_ind[1], .1)\n",
    "                    state = state_arr[new_move_ind]\n",
    "                else:\n",
    "                    self.human_viz(state)\n",
    "                    move_valid = False\n",
    "                    while move_valid == False:\n",
    "                        human_move = input('Input your move coordinates, separated by a comma: ')\n",
    "                        coord_1, coord_2 = int(human_move[0]), int(human_move[-1])\n",
    "                        if state[coord_1, coord_2] != 0:\n",
    "                            print('Invalid move, try again.')\n",
    "                            continue\n",
    "                        tmp_state = np.copy(state)\n",
    "                        tmp_state[coord_1, coord_2] = 2\n",
    "                        if self.check_legal(tmp_state, 3) == True:\n",
    "                            state = tmp_state\n",
    "                            move_valid = True\n",
    "                iter_count += 1\n",
    "\n",
    "        if agent_char is 2:\n",
    "            while self.check_over(state, 3) == 0.5 and iter_count <= 8:\n",
    "                cur_state_ind = self.get_current_state_index(state, state_arr)\n",
    "                if iter_count % 2 != 0:\n",
    "                    move_inds = self.find_next_move(state_arr, state, 2)\n",
    "                    new_move_ind = agent.exploit_learned_probs(move_inds, cur_state_ind[1], .1)\n",
    "                    state = state_arr[new_move_ind]\n",
    "                else:\n",
    "                    self.human_viz(state)\n",
    "                    move_valid = False\n",
    "                    while move_valid == False:\n",
    "                        human_move = input('Input your move coordinates, separated by a comma: ')\n",
    "                        coord_1, coord_2 = int(human_move[0]), int(human_move[-1])\n",
    "                        if state[coord_1, coord_2] != 0:\n",
    "                            print('Invalid move, try again.')\n",
    "                            continue\n",
    "                        tmp_state = np.copy(state)\n",
    "                        tmp_state[coord_1, coord_2] = 1\n",
    "                        if self.check_legal(tmp_state, 3) == True:\n",
    "                            state = tmp_state\n",
    "                            move_valid = True\n",
    "                iter_count += 1\n",
    "\n",
    "        self.human_viz(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt = tictactoe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-85179b34249b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mttt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_4_state_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-508c85f76628>\u001b[0m in \u001b[0;36mgen_4_state_arr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                                                         self.chars[p]]).reshape(4, 4)\n\u001b[0;32m---> 49\u001b[0;31m                                                                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_legal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                                                                             \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-508c85f76628>\u001b[0m in \u001b[0;36mcheck_legal\u001b[0;34m(self, state, n)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_cpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate_cpy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32melif\u001b[0m \u001b[0mstate_cpy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mo_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mstate_cpy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mempty_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arr = ttt.gen_4_state_arr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class td_agent:\n",
    "    def __init__(self, values, p_random_move, alpha_start):\n",
    "        self.vals = values\n",
    "        self.p_random_move = p_random_move\n",
    "        self.alpha = alpha_start\n",
    "    \n",
    "    def alpha_anneal(self, alpha_start, alpha_end, num_iters):\n",
    "        self.alpha -= (alpha_start - alpha_end)/num_iters \n",
    "    \n",
    "    def get_move_update_values(self, move_inds, current_state_ind):\n",
    "        if np.random.binomial(1, self.p_random_move) == 1:\n",
    "            move_ind = np.random.randint(0, len(move_inds))\n",
    "            move = move_inds[move_ind]\n",
    "        else:\n",
    "            greedy_move = np.argmax(self.vals[move_inds])\n",
    "            move = move_inds[greedy_move]\n",
    "            self.vals[current_state_ind] += self.alpha * (self.vals[move] - self.vals[current_state_ind])\n",
    "        \n",
    "        return move\n",
    "    \n",
    "    def exploit_learned_probs(self, move_inds, current_state_ind, learn=False):\n",
    "        move = move_inds[np.argmax(self.vals[move_inds])]\n",
    "        if learn == True:\n",
    "            self.vals[current_state_ind] += self.alpha * (self.vals[move] - self.vals[current_state_ind])\n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = tictactoe()\n",
    "state_arr = game.gen_3_state_arr()\n",
    "\n",
    "x_vals = np.load('/Users/jacobpettit/Documents/reinforcement-learning/BookNotes/x_game_vals_alpha_anneal.npy')\n",
    "o_vals = game.gen_o_values(x_vals)\n",
    "\n",
    "TD_x = td_agent(x_vals, .2, .4)\n",
    "TD_o = td_agent(o_vals, .2, .4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56701349 0.99848589 0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n"
     ]
    }
   ],
   "source": [
    "ts1 = state_arr[0]\n",
    "# win probability should be 1\n",
    "ts2 = np.array([1, 0, 1, 2, 2, 0, 1, 0, 2]).reshape(3, 3)\n",
    "ts3 = np.array([1, 0, 1, 2, 1, 2, 0, 2, 0]).reshape(3, 3)\n",
    "ts4 = np.array([1, 2, 1, 2, 0, 2, 1, 2, 1]).reshape(3, 3)\n",
    "ts5 = np.array([2, 0, 0, 2, 1, 0, 1, 1, 2]).reshape(3, 3)\n",
    "# win probability should be 0\n",
    "ts6 = np.array([2, 0, 2, 0, 1, 1, 2, 1, 1]).reshape(3, 3)\n",
    "ts7 = np.array([2, 2, 0, 1, 0, 0, 1, 0, 1]).reshape(3, 3)\n",
    "ts8 = np.array([2, 0, 2, 1, 2, 1, 1, 1, 0]).reshape(3, 3)\n",
    "ts9 = np.array([1, 1, 0, 1, 2, 0, 2, 0, 0]).reshape(3, 3)\n",
    "ts10 = np.array([1, 1, 0, 0, 1, 0, 2, 2, 0]).reshape(3, 3)\n",
    "\n",
    "ts_inds = []\n",
    "ts_ = [ts1, ts2, ts3, ts4, ts5, ts6, ts7, ts8, ts9, ts10]\n",
    "for i in ts_:\n",
    "    ts_inds.append(game.get_current_state_index(i, state_arr)[1])\n",
    "    \n",
    "print(TD_x.vals[ts_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "X values for test states: [0.56701349 0.5        0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n",
      "O values for test states: [0.43860133 0.5        0.32805    0.5        0.5        0.5\n",
      " 0.5        0.55       0.545      0.6355    ]\n",
      "\n",
      "Iter: 1000\n",
      "X values for test states: [0.56701349 0.5        0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n",
      "O values for test states: [0.43298651 0.5        0.32805    0.5        0.5        0.5\n",
      " 0.5        0.55       0.545      0.6355    ]\n",
      "\n",
      "Iter: 2000\n",
      "X values for test states: [0.56701349 0.99848589 0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n",
      "O values for test states: [0.43298651 0.5        0.32805    0.5        0.5        0.5\n",
      " 0.5        0.55       0.545      0.6355    ]\n",
      "\n",
      "Iter: 3000\n",
      "X values for test states: [0.56701349 0.99848589 0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n",
      "O values for test states: [0.43298651 0.00151411 0.32805    0.5        0.5        0.5\n",
      " 0.5        0.55       0.545      0.6355    ]\n",
      "\n",
      "Iter: 4000\n",
      "X values for test states: [0.56701349 0.99848589 0.67195    0.5        0.5        0.5\n",
      " 0.5        0.45       0.455      0.3645    ]\n",
      "O values for test states: [0.43298651 0.00151411 0.32805    0.5        0.5        0.5\n",
      " 0.5        0.55       0.545      0.6355    ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_wins, o_wins, draws = 0, 0, 0\n",
    "x_win_states, o_win_states, draw_states = [], [], []    \n",
    "for i in range(5000):\n",
    "    iter_count = 0\n",
    "    cur_state = state_arr[0]\n",
    "    while game.check_over(cur_state, n) == 0.5 and iter_count <= 8:\n",
    "        cur_state_ind = game.get_current_state_index(cur_state, state_arr)\n",
    "        if iter_count % 2 == 0:\n",
    "            move_inds = game.find_next_move(state_arr, cur_state, 1)\n",
    "            new_move_ind = TD_x.get_move_update_values(move_inds, cur_state_ind[1])\n",
    "            cur_state = state_arr[new_move_ind]\n",
    "        else:\n",
    "            move_inds = game.find_next_move(state_arr, cur_state, 2)\n",
    "            new_move_ind = TD_o.exploit_learned_probs(move_inds, cur_state_ind[1])\n",
    "            cur_state = state_arr[new_move_ind]\n",
    "        iter_count += 1\n",
    "    if game.check_over(cur_state, n) == 1:\n",
    "        x_wins += 1\n",
    "        x_win_states.append(cur_state)\n",
    "    if game.check_over(cur_state, n) == 0:\n",
    "        o_wins += 1\n",
    "        o_win_states.append(cur_state)\n",
    "    if game.check_over(cur_state, n) == .5:\n",
    "        draws += 1\n",
    "        draw_states.append(cur_state)\n",
    "    TD_x.alpha_anneal(.4, .1, 5000)\n",
    "    if i%1000 == 0:\n",
    "        print('Iter:', i)\n",
    "        print('X values for test states:', TD_x.vals[ts_inds])\n",
    "        print('O values for test states:', TD_o.vals[ts_inds])\n",
    "        print()\n",
    "        np.save('x_game_vals_alpha_anneal.npy', TD_x.vals)\n",
    "        #np.save('o_game_vals_alpha_anneal.npy', TD_o.vals)\n",
    "        TD_o.vals = game.gen_o_values(TD_x.vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '-' '-']\n",
      " ['-' 'O' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' '-']\n",
      " ['-' 'O' '-']\n",
      " ['-' '-' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' '-']\n",
      " ['-' 'O' '-']\n",
      " ['O' 'X' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' 'X']\n",
      " ['-' 'O' 'O']\n",
      " ['O' 'X' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' 'X']\n",
      " ['X' 'O' 'O']\n",
      " ['O' 'X' 'X']]\n"
     ]
    }
   ],
   "source": [
    "game.human_play(state_arr, TD_o, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-']\n",
      " ['-' 'X' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' 'X']\n",
      " ['-' 'X' '-']\n",
      " ['-' 'O' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' 'X']\n",
      " ['-' 'X' '-']\n",
      " ['O' 'O' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '-' 'X']\n",
      " ['-' 'X' 'O']\n",
      " ['O' 'O' 'X']]\n"
     ]
    }
   ],
   "source": [
    "game.human_play(state_arr, TD_x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_x_vals, best_o_vals = np.load('x_game_vals_fixed.npy'), np.load('o_game_vals_fixed.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_x_agent = td_agent(best_x_vals, .2)\n",
    "best_o_agent = td_agent(best_o_vals, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_x_agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-285837109d90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_x_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'best_x_agent' is not defined"
     ]
    }
   ],
   "source": [
    "game.human_play(state_arr, best_x_agent, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-']\n",
      " ['-' '-' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  0,0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' '-' '-']\n",
      " ['O' '-' '-']\n",
      " ['-' '-' '-']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  2,2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' '-']\n",
      " ['O' '-' '-']\n",
      " ['-' '-' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  0,2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' 'X']\n",
      " ['O' 'O' '-']\n",
      " ['-' '-' 'X']]\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input your move coordinates, separated by a comma:  2,0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X' 'O' 'X']\n",
      " ['O' 'O' 'O']\n",
      " ['X' '-' 'X']]\n"
     ]
    }
   ],
   "source": [
    "game.human_play(state_arr, best_o_agent, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43046721"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3**16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "\n",
    "The following is my notes, summary, and paraphrasing of Chapter 2 in Sutton and Barto's Reinforcement Learning: An Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits\n",
    "\n",
    "\"The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that $evaluates$ the actions taken rather than $instructs$ by giving the correct actions.\"\n",
    "\n",
    "If we used feedback that is only evaluating each move, this will tell us how good the action taken was, but not if it was the best or worst action we could've possibly taken.\n",
    "\n",
    "Using feedback that is just instructive will only tell us the right action to take, regardless of the action that we took."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: A $k$-armed Bandit Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem:\n",
    "\n",
    "We repeatedly have to choose from among $k$ different actions. After each action, a reward is given, the reward is picked from an unchanging probability distribution that depends on the action chosen. The goal is to maximize the (expected) total reward over an arbitrary time period.\n",
    "\n",
    "\"This is the original form of the k-armed bandit problem, so named by analogy to a slot machine, or \"one armed bandit\", except that it has $k$ levers instead of one. Each action selection is like a play of one of the slot machine's levers, and the rewards are payoffs for hitting the jackpot. Through repeated action selections you are to maximize your chances of winning by concentrating your actions on the best levers.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the k-armed bandit problem, each action we could choose has an expected or average reward given that action is chosen, we call this the $value$ of that action. \n",
    "\n",
    "Notation:\n",
    "    - Action selected on time step t is A<sub>t<sub/>\n",
    "    - Reward at time step t is R<sub>t<sub/>\n",
    "    \n",
    "So, the value of an action $a$, denoted as $q$<sub>*</sub> $(a)$ is our expected reward given that $a$ is picked.\n",
    "    \n",
    "$q$<sub>*</sub>($a$) = E[$R$<sub>t</sub> | $A$<sub>t</sub> = $a$]\n",
    "    \n",
    "If we knew the values of each action with certainty, it would be easy to solve the k-armed bandit problem, because we would just choose the action with the highest value at each time step.\n",
    "We assume to not know the action values with certainty. At best we may have estimates. The estimated value of action $a$ at time step $t$ is denoted as $Q$<sub>t</sub>($a$).\n",
    "    \n",
    "We want $Q$<sub>t</sub>($a$) to be close to $q$<sub>*</sub>($a$). If we keep estimates of the action values, then at each time step there is an action with the greatest expected value. If we choose the action with that value, these types of actions are considered greedy actions. Choosing one a greedy action means you are exploiting your current knowledge of action values. \n",
    "    \n",
    "Nongreedy actions are said to be exploring actions, because we are improving our estimate of the nongreedy action's value. \n",
    "    \n",
    "\"Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater reward in the long run.\"\n",
    " \n",
    "Since we can't possibly explore and exploit with one action, there's a balance to be struck between exploration and exploitation.    \n",
    "\n",
    "In each specific case, if it is better to explore or exploit depends on a multitude of factors; the values of the estimates, uncertainties, and the number of steps left. Lots of different, sophisticated, methods exist to balance between exploration and exploitation for certain formulations of the k-armed bandit problem. Issue is, most of these methods make \"strong assumptions about stationary and prior knowledge that are either violated or impossible to verify in applications and in the full reinforcement learning problem that we consider in subsequent chapters.\"\n",
    "    \n",
    "In chapter 2 we will cover several simple balancing methods for the k-armed bandit problem and we will see that they are superior to methods that always exploit. Balancing exploration and exploitation is a \"distinctive challenge\" in RL; our simple version of the k-armed bandit problem will allow us to show this in a very clear way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Action-Value Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural way to estimate the true value of an action is to average the rewards received:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$<sub>$t$</sub>($a$) = $\\frac{\\sum_{i=1}^{t - 1}R_i * 1}{\\sum_{i=1}^{t-1} 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $1$ is the same as $1_{predicate}$ in the textbook.\n",
    "\n",
    "From text: the 1 in the formula above should be *1<sub>A<sub>i</sub>=a</sub>*\n",
    "\n",
    "$1_{predicate}$ denotes a random variable that is 1 if $predicate$ is true and 0 if it isn't. If the denominator becomes zero, then we define $Q_t(a)$ by some other value, like 0. As the denominator grows towards infinity, by the law of large numbers, $Q_t(a)$ converges towards $q_*(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called the sample-average method; estimates action values because each estimate is average of the sample of relevant rewards. \n",
    "\n",
    "Can write a greedy action selection method as: $A_t = argmax  (Q_t(a))$. Greedy action selection *always* exploits current knowledge to maximize the immediate reward. It does not try at all to sample inferior actions and discover if they might actually be better.\n",
    "\n",
    "An easy way to most often behave greedily, but then with a small probability, call it $\\varepsilon$, randomly sample from all of the moves with equal probability. Ignore the action-value estimates. Methods like this are called $\\varepsilon-greedy$ methods. With these methods, as the number of steps increases, every action will be sampled an infinite number of times, guaranteeing that all the $Q_t(a)$ converge to $q_*(a)$. This implies that the probability of selecting the optimal action converges to more than $1 - \\varepsilon$, which is near certainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The 10-armed Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the effectiveness of the greedy and $\\varepsilon - greedy$ methods, the book authors compared them numerically on a bunch of test problems. It was a set of 2000 randomly generated k-armed bandit problems where k = 10. For each bandit problem, the action values $q_*(a)$; a = 1, 2, 3, ..., 10 were sampled from a normal distribution with mean 0 and variance 1. When a learning method applied to a problem picked action $A_t$, the reward $R_t$ was sampled from a normal distribution with mean $q_*(A_t)$ and variance 1. They call this set of tasks the 10 armed testbed.\n",
    "\n",
    "For any learning method, they can measure its performance and behavior as it learns and gains experience over 1000 time steps on a particular bandit problem. This makes up one run. \n",
    "\n",
    "They repeated this for 2000 runs, each with a different bandit problem, and so obtained measures of the learning algorithm's average behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot in the textbook, figure 2.2, shows that a greedy-only method will most quickly maximize reward in the short term, but in the long term it will get less reward than a $\\varepsilon$ greedy method. The method with $\\varepsilon = 0.1$ rapidly maximized reward, and continued to maximize reward over the long run. When $\\varepsilon$ is 0.01, it more slowly maximizes reward but if it is allowed to run for long enough, it will eventually perform better than when $\\varepsilon = 0.1$. A second plot shows that a greedy-only method only learns to make the optimal move about a third of the time, while the $\\varepsilon = 0.1$ method learns to make the optimal move about 85% of the time. Again, $\\varepsilon=0.01$ is slower to converge, but given enough time it will outperform $\\varepsilon=0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Incremental implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action-value methods covered so far all estimate values as sample averages of the observed rewards. Now we look at how these averages can be computed efficiently, with constant memory and constant per-time-step computation.\n",
    "\n",
    "To help make notation simpler, we look at a single action. Have $R_i$ denote the reward received after the $i$th selection of this action and let $Q_n$ be the estimate of the action value after it has been picked $n-1$ times.\n",
    "\n",
    "We can now write the $Q_n$ as:\n",
    "\n",
    "$Q_n \\stackrel{.}{=} \\frac{R_1 + R_2 + \\dotsb + R_n}{n-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An obvious implementation would be to have an array keeping a record of all the rewards and then perform the above computation whenever the estimated value of the action was needed. But, if we do it this way, then memory and computational requirements would grow over time as more reward is seen. Each additional reward would need more memory to store it and it would add more time to the calculation.\n",
    "\n",
    "It isn't really necessary to do it this way. We can come up with incremental formulas to update the averages with small, constant amount of computation required to process each reward.\n",
    "\n",
    "Given $Q_n$ and the nth reward $R_n$, the new average of all n rewards can be found with:\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}\\sum_{i=1}^{n}R_i$\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}(R_n + \\sum_{i=1}^{n-1}R_i)$\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}(R_n + (n-1)\\frac{1}{n-1}\\sum_{i=1}^{n}R_i)$\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}(R_n + (n-1)Q_n)$\n",
    "\n",
    "$Q_{n+1} = \\frac{1}{n}(R_n + nQ_n - Q_n)$\n",
    "\n",
    "$Q_{n+1} = Q_n + \\frac{1}{n}[R_n - Q_n]\\text{    }$    (Eqn. 2.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This holds even for n = 1, giving $Q_2 = R_1$ for arbitrary $Q_1$. It requires memory for only $Q_n$ and $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psuedocode for a simple bandit algorithm:\n",
    "\n",
    "for a = 1:k\n",
    "\n",
    "$Q(a) \\leftarrow 0$\n",
    "\n",
    "$N(a) \\leftarrow 0$\n",
    "\n",
    "Do forever:\n",
    "\n",
    "$A \\leftarrow\n",
    "    \\begin{cases}\n",
    "    argmax_a(Q(a)) & \\quad \\text{with } \\text{probability } 1 - \\varepsilon \\text{ (breaking ties randomly)}\\\\\n",
    "    \\text{a random action} & \\quad \\text{with } \\text{probability } \\varepsilon\n",
    "    \\end{cases}\n",
    "$\n",
    "\n",
    "$N(A) \\leftarrow N(A) + 1$\n",
    "\n",
    "$Q(A) \\leftarrow Q(A) \\frac{1}{N+A}[R - Q(A)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule (2.3) has a form that occurs frequently. The form is:\n",
    "\n",
    "$\\text{NewEstimate } \\leftarrow \\text{OldEstimate } + \\text{StepSize}[\\text{Target } - \\text{OldEstimate}]$\n",
    "\n",
    "The expression $[\\text{Target } - \\text{OldEstimate}]$ is an error in the estimate. We reduce the error by taking a step toward the target. Target is presumed to be the desirable direction to move towards, but it may be noisy. \n",
    "\n",
    "The step size used in the method above changes from time step to time step. When getting the nth reward for action $a$, the method uses the step-size paramater $\\frac{1}{n}$. Later on we denote this by $\\alpha$ or, more generally, $\\alpha_t(a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Tracking a Nonstationary Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaging methods discussed so far are appropriate for stationary problems, where the reward probabilities don't change over time. Often we are dealing with a nonstationary problem. When this is the case it makes sense to give more weight to the more recent rewards than to rewards from longer ago. \n",
    "\n",
    "One of the most popular ways to do this is to use a constant step-size parameter. The incremental update rule (2.3) is modified to be:\n",
    "\n",
    "$Q_{n+1} \\stackrel{.}{=} Q_n + \\alpha * [R_n - Q_n]$\n",
    "\n",
    "With step size parameter $\\alpha \\in (0, 1]$ is constant. This makes $Q_{n+1}$ be a weighted average of past rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial estimate $Q_1$:\n",
    "\n",
    "$Q_{n+1} = Q_n + \\alpha[R_n - Q_n]$\n",
    "\n",
    "$Q_{n+1} = \\alpha R_n + (1 - \\alpha)Q_n$\n",
    "\n",
    "$Q_{n+1} = \\alpha R_n + (1 - \\alpha)[\\alpha R_{n-1} + (1 - \\alpha)Q_{n-1}]$\n",
    "\n",
    "$Q_{n+1} = \\alpha R_n + (1 - \\alpha)\\alpha R_{n-1} + (1 - \\alpha)^2\\alpha R_{n-2} + \\dotsb + (1 - \\alpha)^{n-1} \\alpha R_1 + (1 + \\alpha)^nQ_1$\n",
    "\n",
    "$Q_{n+1} = (1 - \\alpha)^nQ_1 + \\sum_{i=1}^{n} \\alpha(1-\\alpha)^{n-i}R_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a weighted average because the sum of the weights is $(1-\\alpha)^n + \\sum_{i=1}^{n}\\alpha(1-\\alpha)^{n-i} = 1$. The weight $\\alpha(1-\\alpha)^{n-i}$ given to the reward $R_i$ depends on how many rewards ago, $n-i$, it was observed. The quantity $1-\\alpha$ is less than one and so the weight of $R_1$ decreases as the number of intervening rewards increases. The weight decays exponentially according to the exponent on $1-\\alpha$. If $1-\\alpha = 0$ then all of the weight goes onto the very last reward, $R_n$, because $0^0 = 1$. This is sometimes called an *exponential receny-weighted average*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to vary the step-size parameter from step to step. Let $\\alpha_n(a)$ denote the step-size parameter used to process the reward received after the nth selection of action $a$. From above, the choice of $\\alpha_n(a) = \\frac{1}{n}$ is the sample-average method. The sample-average method is guaranteed to converge to the true action values by the law of large numbers. But it is not guaranteed to converge for all choices of the sequence $\\alpha_n(a)$. The conditions required to assure convergence with probability 1:\n",
    "\n",
    "$\\sum_{n = 1}^{\\infty} \\alpha_n(a) = \\infty$ \n",
    "\n",
    "and\n",
    "\n",
    "$\\sum_{n = 1}^{\\infty} \\alpha_n^2(a) < \\infty$\n",
    "\n",
    "The first condition is required to guarantee that the steps are large enough to overcome any initial conditions or random fluctuations. The second condition is required to ensure that the steps are small enough to assure convergence.\n",
    "\n",
    "Both conditions are met for the sample-average case ($\\alpha_n(a) = \\frac{1}{n}$) but not for the constant step size parameter ($\\alpha_n(a) = \\alpha$). In the constant case, the second condition isn't met. So the estimates never completely converge but instead continue to vary in response to the latest rewards. This is desirable for a nonstationary environment. And nonstationary problems are the most common in reinforcement learning. Additionally, sequences of step-sizes that meet the conditions often converge slowly or require lots of tuning to get a satisfactory rate of convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Optimistic Initial Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each method discussed so far is dependent to some extent on the initial action-value estimates, $Q_1(a)$. These methods are biased by their intial estimates. For the sample-average methods, the bias disappears once all of the actions have been selected at least one time. But for methods with constant $\\alpha$, the bias is permanent. In practice, the bias isn't a problem and can sometimes be helpful. Downside is that the intial values become practically another set of parameters that must be set by the user. The upside is that they give a simple way to provide some prior knowledge about what kinds of rewards can be expected. \n",
    "\n",
    "Initial action values can also be used to encourage exploration. Suppose instead of setting initial values to 0, like in the 10-armed testbed, we set them to +5. The $q_*(a)$ in that problem were selected from a normal distribution with mean 0 and variance 1, so an intial estimate of +5 is extremely optimistic. The optimism encourages action-value methods to explore. Whichever actions are chosen initially, the reward is certainly less than +5 and so the agent switches to other actions. This makes all actions be tried several times before the value estimates converge. And a great deal of exploration occurs even if greedy actions are always chosen. \n",
    "\n",
    "Figure 2.3 in the book shows that a greedy method using $Q_1(a) = +5$ outperforms the $\\varepsilon-greedy$ method in the 10-armed bandit testbed. This technique for encouraging exploration is called *optimistic initial values*. It is regarded as a simple trick for encouraging exploration that can be effective on simple problems. But it is not a generally useful approach to encouraging exploration. It is poorly suited to nonstationary problems because its drive for exploration is inherently temporary. The second the task changes and produces a renewed need for exploration, this method can't help with that. \n",
    "\n",
    "Any method that focuses on the initial conditions in any special way is unlikely to help with a nonstationary case. Since the start of time happens only one time, too much focus shouldn't be placed on it. This same criticism applies to the sample-average methods, since these also treat the beginning of time as a special event and average all subsequent rewards with  equal weights. Nonetheless, each of these methods is very simple, and one of them or a simple combination of them is often enough in practice. Throughout the rest of the book they will make frequent use of several of the simple exploration techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Upper-Confidence-Bound Action Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
